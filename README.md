# ğŸš“ Racial Bias in U.S. Traffic Stops â€” Outcome-Based Predictive Modeling

## ğŸ“Œ Overview

This project investigates **systemic racial bias in police traffic stops** using a novel **outcome-sensitive predictive modeling approach**. Rather than analysing stop frequency or hit rates, it tests whether **the outcome of a stop** (e.g., citation, arrest) can be predicted using **race-neutral features**, and how prediction accuracy changes when race is included.

> Conducted as one of two capstone research projects during the **Master of Data Analysis** at **Queensland University of Technology (QUT)**.

---

## ğŸ‘¤ Author

**Alex Conroy**  
Master of Data Analysis â€” QUT  
ğŸ“š Independent Research Dissertation  
ğŸ“ Project 1 of 2

---

## ğŸ¯ Project Goal

> Can we model the **decision to arrest or cite a driver** without knowing their race â€” and does model accuracy improve if race is included?

This reframes the problem from stop bias to **decision-making sensitivity**, helping avoid confounders present in traditional hit rate or stop frequency analysis.

---

## ğŸ“Š Data Sources

- **Stanford Open Policing Project (SOP):** Aggregated traffic stop data across U.S. states
- **U.S. Census Bureau ZCTA Crosswalk:** Demographic context by ZIP code
- **Google Maps Geocoding API:** Reverse-geocoding of stop locations (ZIP â†’ ZCTA)

---

## ğŸ§  Methodology

### ğŸ“ Geospatial Enrichment (Python)
- Reverse geocoded ZIP codes to ZCTA using Google Maps API
- Merged with U.S. Census population and race/ethnicity data

### ğŸ“ˆ Statistical Modeling (R)
- Constructed multinomial logistic regression models using `nnet::multinom()`
- Compared outcome prediction accuracy with and without racial features
- Evaluated across states (e.g. Colorado, North Carolina) and time windows

---

## ğŸ’¡ Key Results

| Metric                 | With Race | Without Race |
|------------------------|-----------|---------------|
| Predictive Accuracy    | â†‘ Improved | â†“ Decreased   |
| State-Level Consistency| High       | High          |
| Interpretability       | Transparent| Transparent   |

âœ… Racial identifiers **increase predictive power**, implying bias in outcome decisions  
âœ… Shows **decision-level sensitivity** to race, not just stop frequency bias  
âš ï¸ Ethical limitation: models reflect *outcomes*, not necessarily officer intent

---

## ğŸ” Project Structure
racial-bias-traffic-stops/ â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ sop_subset.csv # Subsampled SOP stop data â”‚ â””â”€â”€ census_zcta_mapping.csv # ZIP-to-ZCTA crosswalks â”‚ â”œâ”€â”€ notebooks/ â”‚ â”œâ”€â”€ zcta_cluster_analysis.ipynb # Geospatial and census merging (Python) â”‚ â”œâ”€â”€ reverse_geo_code.ipynb # Google Maps geocoder (Python) â”‚ â”œâ”€â”€ analysis/ â”‚ â””â”€â”€ SOPModel_all_test.Rmd # Multinomial regression modeling (R) â”‚ â”œâ”€â”€ report/ â”‚ â”œâ”€â”€ Report DRAFT.pdf # Draft dissertation write-up â”‚ â”œâ”€â”€ IFN704_Project_Proposal.pdf # Initial research proposal â”‚ â””â”€â”€ IFN704_Alex_Conroy_Presentation.pdf # Final presentation slides â”‚ â”œâ”€â”€ research.txt # Raw notes and research log â”œâ”€â”€ README.md


---

## ğŸ§ª Tools & Libraries

- **Python:** pandas, requests, geopy, geocoding APIs  
- **R:** `nnet`, `car`, `dplyr`, `ggplot2`  
- **Jupyter + RMarkdown**: Mixed-language data science pipeline  
- **PDF Reporting:** Academic communication and presentation

---

## âš ï¸ Limitations

- Models **cannot infer causality or intent**
- Data only includes **recorded stop outcomes**, not reasoning
- Geocoding errors or data gaps may introduce spatial noise
- SOP data completeness varies by state

---

## ğŸ”® Future Work

- Expand to additional states or newer SOP releases  
- Apply **causal inference** or counterfactual analysis  
- Integrate police bodycam metadata or context from court outcomes  
- Develop fairness-aware models using **sensitive feature auditing**

---

## ğŸ“„ License

This project uses public data from the **Stanford Open Policing Project** and the **U.S. Census Bureau**.  
No proprietary or confidential datasets were used.

---

## ğŸ† Why This Matters

This project shows how **transparent, explainable modeling** and thoughtful geospatial analysis can support conversations around **bias, ethics, and decision fairness** â€” essential for data science in government, policy, and social impact sectors.






